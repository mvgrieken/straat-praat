AI-module (vertaalservice): Onder de motorkap maakt de vertaalfunctie gebruik van een AI-taalmodel om lastige vertalingen (vooral hele zinnen of onbekende slang) intelligent af te handelen. We hebben een dedicated vertaaldienst opgezet (bijvoorbeeld als aparte microservice) die via een API aangesproken wordt door de app/back-end. Deze service gebruikt een Large Language Model (LLM) zoals OpenAI’s GPT-4 of Anthropic’s Claude, of een eigen getraind model, gespecialiseerd in Nederlandse Straat-Praat. De prompt (instructie) voor dit model is zorgvuldig ontworpen zodat het model consistente en veilige vertalingen geeft. Het model wordt bijvoorbeeld geïnstrueerd met een systeemboodschap in de trant van: “Je bent een vertaalassistent die Nederlandse Straat-Praat omzet in formeel Nederlands en vice versa. Als de gebruiker slang invoert, geef de nette vertaling; als de gebruiker formeel Nederlands invoert, geef een informele slang-vertaling. Behoud de betekenis en toon, gebruik veelvoorkomende jongerentaal. Als een slangterm grof of beledigend is, vertaal die naar een neutrale omschrijving (geen nieuwe scheldwoorden toevoegen). Als je een term niet kent of twijfelt, geef dat aan (bijv. ‘(?)’ of zeg ‘mogelijk betekent dit X’) in plaats van iets te verzinnen. Geef het antwoord kort en bondig, idealiter alleen de vertaling.” Deze instructies zorgen dat het AI-model zich gedraagt als pure vertaler en niet afdwaalt of ongewenste output geeft. Omdat Straat-Praat soms schuttingwoorden bevat, hebben we expliciet gemaakt dat het model wel moet vertalen wat er staat (dus niet zomaar weigeren), maar dat het resultaat in net Nederlands mag omschrijven dat het om een scheldwoord gaat. Onbekende slang moet het model zo goed mogelijk uit context proberen te raden, maar mét een kenmerk van onzekerheid, zodat de gebruiker gewaarschuwd is dat het wellicht niet klopt. Bijvoorbeeld als iemand invoert “Die outfit is fleek”, en het model is niet 100% zeker, zou het kunnen antwoorden: “Die outfit is geweldig (vermoedelijk betekenis)” – dus het geeft een vertaling maar laat zien dat het de slang “fleek” niet volledig kent. Dit voorkomt dat er hallucinaties komen (het model iets volledig verkeerds verzint) en is onderdeel van het veilig prompten. Bij veel LLM’s is het zelfs nodig te benadrukken dat ze best “ik weet het niet” mogen zeggen – we hebben dat in de prompt gezet, zodat de AI niet met onzin komt maar eerlijk aangeeft als iets onbekend is. API-endpoints: De app/backend communiceert met deze AI-module via een paar REST API endpoints. Het belangrijkste is POST /translate. De client stuurt een JSON met de tekst om te vertalen en de doeltaalmodus. Bijvoorbeeld { "text": "Mag ik een vuurtje?", "target": "slang" } om die zin naar Straat-Praat om te zetten, of {"text": "Die chick is mijn fam", "target": "formal"} om slang naar normaal te gaan. We gebruiken expliciet een veld “target” met waarde "formal" of "slang" zodat de service niet zelf hoeft te raden wat de bedoeling is – de app weet immers in welke modus de gebruiker zit. (Alternatief hadden we een automatische detectie kunnen doen, maar dat is foutgevoelig, dus we kiezen voor expliciete modus of twee aparte endpoints /toSlang en /toFormal.) Het antwoord van /translate is ook JSON, bijvoorbeeld: { "translation": "Die meid hoort bij mijn familie", "confidence": 0.95 }. Hierin is translation de vertaalde tekst. We geven ook een confidence mee – een getal tussen 0 en 1 (95% zeker in dit voorbeeld) dat aangeeft hoe zeker de AI van zijn zaak is. Dit kan ook een kwalitatieve waarde “hoog/middel/laag” zijn. Desgewenst kan er nog een veld notes zijn waarin de service bijzonderheden noteert (bijv. “translated slang -> formal, offensive term toned down” of onzekerheid bij bepaalde woorden). In een minimale implementatie volstaan echter de vertaling en een confidence score. Het tweede endpoint is POST /feedback. Dit is bedoeld om gebruikersfeedback te registreren. De mobiele app kan na het tonen van een vertaling bijvoorbeeld vragen “Klopt deze vertaling?” met duim omhoog/omlaag. Als de gebruiker “incorrect” aangeeft, stuurt de app een call naar /feedback met bijv. {"original": "...", "translation": "...", "feedback": "incorrect"} (of een ID dat de vertaalactie identificeert, plus een vlag). De server kant slaat deze feedback op in een log of database voor latere analyse. Er is ook een simpel endpoint GET /health voorzien voor monitoring, dat gewoon “ok” teruggeeft als de service draait – zo kunnen we automatisch checken of de vertaaldienst online is. De communicatie met de AI-service gebeurt over HTTPS en in JSON formaat, wat gemakkelijk te parseren is in de app. We zorgen dat zowel input als output UTF-8 encodering ondersteunt, zodat ook emoji’s of bijzondere tekens in slang (denk aan “👊” of zo) goed doorkomen. Foutafhandeling in de vertaaldienst: We hebben meerdere lagen van foutafhandeling. Als de gebruiker een ongeldige aanvraag stuurt (bijv. JSON mist het text veld), retourneert /translate een 400 Bad Request met een duidelijke foutmelding in JSON, zodat de app eventueel aan de gebruiker kan melden dat er iets mis is met de invoer. Als de gebruiker extreem veel tekst invoert (meer dan de AI aan kan, zeg een heel lang verhaal), kan de service ervoor kiezen alleen de eerste zoveel karakters te vertalen of een 413 Payload Too Large fout terug te geven met de melding dat de tekst te lang is. Als de AI API van OpenAI/Claude zelf een error geeft – bijvoorbeeld een timeout, of de API-key is rate-limited – dan zal onze service dat opvangen. Er wordt eventueel één retry gedaan als het een timeout betreft. Als het echt niet lukt, geven we een 500 Internal Server Error terug aan de app. De app kan daarop bijvoorbeeld een generieke melding tonen (“Vertalen is momenteel niet beschikbaar, probeer het later nog eens.”). Belangrijk is dat zulke fouten gelogd worden met details, zodat ontwikkelaars weten dat er iets misging (bijv. “OpenAI API timeout at 15s”). Een ander scenario: het model geeft een antwoord terug dat niet in het verwachte formaat is. Ondanks onze instructies zou een LLM kunnen terugpraten (“Sure, I can translate that for you: ...”). Daarom hebben we validatie: als we JSON verwachten maar de respons is geen geldige JSON, probeert de backend de tekst te parseren of te corrigeren. We kunnen ook van de OpenAI API gebruikmaken van function calling of een verplicht JSON schema, zodat het model direct gestructureerd antwoordt. In het ideale geval dwingt dat het juiste formaat af. Als fallback als het toch niet helemaal klopt, kan de code proberen relevante stukken uit de tekst te halen. Mocht alles falen, dan kan de service eventueel nog een tweede poging doen met een hardere prompt (“antwoord alleen met JSON!”) of als uiterste een andere aanpak gebruiken (bijvoorbeeld een kleinere eigen vertaalslag of een statische woordenlijst als backup voor enkelvoudige woorden). Het belangrijkste is dat de app uiteindelijk iets van antwoord krijgt – desnoods een foutmelding – en niet blijft hangen. Geen vertaling gevonden: In zeldzame gevallen weet het model echt geen raad (de zin is bijv. onzin of bevat slang die zelfs na gok onbekend blijft). We hebben het model zo getraind dat het in zulke gevallen iets van onzekerheid zegt. Als de confidence heel laag zou zijn (onder een drempel, bijv. 0.2), kunnen we overwegen in plaats van een normaal antwoord een speciale melding terug te geven, zoals translation: "(Kon niet vertaald worden)" met confidence 0, zodat de app dat netjes kan tonen. Maar meestal zal het model zelf al iets van “(?)” in de output zetten bij grote twijfel, en geven we dat gewoon door. Confidence score bepaling: Zoals genoemd krijgt de gebruiker bij een vertaling een indicatie hoe betrouwbaar de vertaling is. Omdat een model als GPT niet vanzelf een waarschijnlijkheidspercentage geeft, lossen we dit via de prompt en heuristiek op. We vragen het model bijvoorbeeld om aan het eind van de vertaling tussen haakjes een trefwoord te zetten zoals (zekerheid: hoog), (zekerheid: middel) of (zekerheid: laag). De backend herkent dat en slaat het om naar een numerieke waarde (bijv. hoog = 0.9, middel = 0.5, laag = 0.2). Als we direct JSON laten genereren, kunnen we het model laten invullen "confidence": 0.xx op basis van zijn inschatting. Die inschatting baseert het model op hoe “vreemd” of onzeker de input klinkt. We ondersteunen dit eventueel met eigen logica: als de output bepaalde woorden zoals “misschien” of vraagtekens bevat, dan verlagen we de confidence. Ook kijken we of alle belangrijke woorden uit de input terugkomen in de output (als het model hele stukken weglaat, is dat verdacht). In toekomstige versies zouden we een geavanceerdere aanpak kunnen doen, bv. de prompt twee keer naar het model sturen en kijken of dezelfde vertaling terugkomt: als ja, dan vrij zeker; als elke keer iets anders, dan onzeker. Maar dat kost meer API-calls en doen we alleen als performance het toelaat. Voor nu vertrouwen we op de zelfinschatting van het LLM en eenvoudige checks. De confidence wordt in de JSON meegegeven aan de app, die er bijv. een icoontje (groen/oranje/rood lampje) van kan maken of een tekst “(onzeker)” kan tonen. Zo weet de ouder of hij de vertaling met een korreltje zout moet nemen. Leermechanisme en modelaanpassing: Deze AI-vertaler kan gaandeweg slimmer worden. We voorzien een paar dingen. Ten eerste een dynamisch woordenboek op de backend: stel meerdere gebruikers geven aan dat de vertaling voor een bepaald nieuw slangwoord telkens incorrect is, dan kan een taalexpert dat slangwoord met de juiste betekenis toevoegen aan een interne lijst. De vertaalmodule kan die lijst raadplegen bij iedere aanvraag. Concreet: als de gebruiker “ossow” invoert en dat woord staat in ons woordenboek als “ossow = geld (Straat-Praat, vervorming van ‘osso’ Surinaams voor huis, in context betekent het geld)”, dan kunnen we deze kennis in de prompt toevoegen (“NB: ‘ossow’ betekent geld.”) zodat het model het zeker goed vertaalt. Zo’n woordenlijst kan gevuld worden met alle slang die het model (nog) niet goed doet. Daarnaast verzamelen we via de genoemde feedbackfunctie een dataset van voorbeeldzinnen en de correcte vertalingen. Op termijn kunnen we het AI-model hiermee fine-tunen: we kunnen bijv. OpenAI’s GPT-3.5 fine-tunen met honderden paaren slang→normaal en normaal→slang in het Nederlands. Ook een opensource model (bv. mT5 of BLOOM) zouden we zelf kunnen trainen op die data. Fine-tuning zou de vertalingen nog accurater en sneller kunnen maken voor dit domein. We zorgen er wel voor dat door fine-tunen het model niet té eenzijdig wordt – het moet algemeen Nederlands nog kunnen, maar aangevuld met extra kennis van jongerentaal. Deze modelupdates hoeven niet continu te gebeuren; misschien eens in de zoveel tijd als er genoeg nieuwe data is. Reinforcement learning (bijv. een RLHF-proces waarbij het model leert van feedbackscores) hebben we geopperd maar dat is complex voor nu – we houden het bij supervised learning van verzamelde correcte voorbeelden. Feedbackloop in praktijk: Alle vertaalverzoeken en feedback worden gelogd (met respect voor privacy, zie hieronder). We kunnen uit de logs bijvoorbeeld halen dat het model moeite had met woord X omdat dat steeds een lage confidence gaf of altijd “laag” werd beoordeeld. Die inzichten gebruiken we om het prompt aan te passen of woorden toe te voegen aan het woordenboek, of bij fine-tuning extra nadruk op die voorbeelden te leggen. Zo wordt de vertaling steeds beter afgestemd op Straat-Praat. Logging & Privacy: In de AI-service loggen we belangrijke informatie voor debugging: iedere inkomende request (zonder de volledige zin als dat privacygevoelig is – of we maskeren bepaalde woorden zoals namen), de uitgaande prompt naar de AI en het antwoord (zodat ontwikkelaars kunnen zien wat het model deed en of het format klopte), en de feedback die terugkomt met een koppeling naar de oorspronkelijke request (bijv. via een ID). We anonimiseren waar mogelijk: mochten gebruikers volledige zinnen invoeren die persoonlijke informatie bevatten (“Hoi ik ben Jan en woon in …”), dan kunnen we proberen namen/adres te detecteren en in de log te vervangen door placeholders. We gebruiken de AI-service uitsluitend voor de vertaalfunctie; er wordt geen andere gebruikersdata naar derden gestuurd. Volgens OpenAI’s beleid wordt API-verkeer niet gebruikt om hun modellen te trainen en verwijderen zij de data na 30 dagen automatisch, wat gunstig is voor privacy. Als extra kunnen we bij OpenAI een data opt-out aanvragen zodat ze de data helemaal niet opslaan. In onze eigen database houden we de vertalingen en feedback mogelijk iets langer, maar ook dat is vooral om de service te verbeteren. We slaan bij voorkeur geen persoonsidentificerende gegevens op bij deze logs (dus geen account ID, of indien wel dan gehashed). In de app-informatie (privacyverklaring) wordt duidelijk vermeld dat gebruikersinvoer naar een externe AI-service gaat voor vertaling, en dat we anonieme feedback gebruiken om de vertaalfunctie te verbeteren. De hele communicatie vindt versleuteld plaats. Alleen geautoriseerde ontwikkelaars kunnen de logbestanden inzien, en ook dan alleen voor het doel van kwaliteitsverbetering. Samengevat zorgt de AI-module ervoor dat de vertaalfunctie van losse woorden uitbreidt naar zinnen en moeilijkere constructies, met behoud van een veilig en transparant karakter (onzekerheden worden aangegeven, ongepaste taal wordt geneutraliseerd, privacy van de gebruiker wordt gerespecteerd).

Automatische Dataverzameling (scraper): Naast bijdragen van gebruikers haalt het systeem ook proactief nieuwe slangwoorden van het internet. Er is een Python-script ontwikkeld dat periodiek publieke bronnen afstruint op zoek naar Straat-Praat. Dit script kan bijvoorbeeld dagelijks draaien (via een cronjob of scheduler) en kijkt naar platforms waar jongeren taalgebruik delen. Enkele bronnen die het script gebruikt: Reddit (specifieke subreddits zoals r/teenagers of r/Straat-Praat als die bestaat), misschien TikTok- of Twitter-API’s voor trending woorden (voor zover beschikbaar), en online slangwoordenboeken zoals Urban Dictionary voor Nederlandse inzendingen. De aanpak is modulair: voor elke bron is er een functie. Bijvoorbeeld fetch_slang_from_reddit haalt de nieuwste posts op van bepaalde subreddits via de Reddit JSON API en filtert de woorden daarin. Het script extraheert tekst uit posts (titel en inhoud) en splitst deze in woorden. Dan filtert het op woorden die waarschijnlijk slang zijn: bv. woorden langer dan 2 letters die niet in een standaard stopwoordenlijst voorkomen en die bepaalde kenmerken hebben (bijv. veelvoorkomende slangpatronen of afkortingen). Elk kandidaat-slangwoord onderwerpt het script aan twee controles voordat het geaccepteerd wordt: duplicaten en moderatie. Via Supabase (die onze PostgreSQL database achter de schermen is) checkt het script of het woord al in de woordenlijst staat (of al eerder gevonden is). Zo niet, dan gebruikt het de OpenAI Moderation API of een eigen lijst met verboden termen om te bepalen of het woord niet extreem ongepast of toxisch is. We willen wel echte slang, inclusief eventueel grove taal, maar we filteren bijvoorbeeld racistische termen of puur scheldwoorden zonder verdere waarde. Als het woord door deze filters komt, voegt het script het toe aan de “new_words” tabel in de database. Hierbij slaat het de volgende velden op: het woord zelf (de slangterm), de bron waar het gevonden is (bijv. "reddit/r/teenagers" of "UrbanDictionary" met eventueel een link), de datum waarop het gevonden is, en een stukje context (bijvoorbeeld een korte zin of het zinsfragment waarin het woord voorkwam, om later de betekenis te kunnen afleiden). Deze context kan moderators helpen begrijpen hoe het woord gebruikt werd. Het script logt zijn acties (aantal woorden gevonden, fouten bij verbinding etc.) zodat we in productie in de gaten kunnen houden of het werkt. Het is robuust gemaakt: als een bron tijdelijk niet reageert, wordt dat gelogd maar het script gaat door met de andere bronnen. Door de modulaire opzet kunnen we later makkelijk nieuwe bronnen toevoegen (stel er komt een openbare API van TikTok hashtags of YouTube comments). Dit geautomatiseerde verzamelen zorgt dat nieuwe slang die online in omloop komt snel op onze radar staat. Vervolgens kan een contentbeheerder deze binnengehaalde woorden bekijken (vergelijkbaar met community-inzendingen) en met één klik overzetten naar de officiële woordenlijst als ze legit zijn, inclusief het invoeren van de juiste definitie. Zo blijft de app up-to-date met minimale handmatige speurwerk. De eerste versie van het script richt zich op Reddit en Urban Dictionary, waar veel jongerentaal te vinden is. Het gebruikt libraries als requests (voor API calls), BeautifulSoup (voor HTML parsing indien nodig bij webpagina’s), en de Supabase Python client om direct in onze database te schrijven. Om die databasehandelingen veilig te doen, gebruikt het script een service-role API key van Supabase (alleen op de server, nooit in de app) waarmee het rechten heeft om de tabel bij te werken. De OpenAI API key voor moderatie wordt ook server-side gebruikt. We zorgen dat al deze keys als environment variables zijn ingesteld en niet hardcoded in het script. Dit dataverzamelscript draait bij voorkeur op een server of cloud function. In de logging meldt het bijvoorbeeld “Verbonden met Supabase database”, “15 nieuwe mogelijke slangwoorden gevonden op reddit, 3 toegevoegd, 12 gefilterd (7 duplicaten, 5 ongepast)”. Zo kunnen ontwikkelaars of beheerders het proces monitoren. Samengevat automatiseert dit script de eerste stap van contentverrijking: nieuwe slang opsnorren in het wild, filteren en klaarzetten in de database, zodat de contentmanager ze alleen nog maar hoeft te beoordelen en in te voegen.

Database & Zoeklogica (data-verrijking): De centrale woordenlijst in de database is slim gestructureerd om efficiënte zoekresultaten en flexibiliteit voor varianten te bieden. We hebben een PostgreSQL schema (via Supabase) met twee hoofdtabellen: words en word_variants. De words tabel bevat de lemma’s – dat zijn de hoofdvormen van elk slangwoord. Bijvoorbeeld de basisvorm “bro” (van brother) zou een record in words zijn, met kolommen: word (“bro”), meaning (“vriend, broer (als aanspreekvorm)”), example (“Bijv: ‘Hey bro, alles goed?’”), audio_url (link naar de audiofile als die er is), en metadata zoals created_at. Belangrijk: er zijn ook kolommen voor normalized_word en phonetic_primary / phonetic_secondary. Dit zijn velden voor zoekoptimalisatie. Normalized_word is het resultaat van een normalisatiefunctie die we op het woord toepassen: het woord wordt in lowercase omgezet, accenten verwijderd, speciale tekens eruit gefilterd, dubbele of langere herhalingen van letters gereduceerd (bijvoorbeeld “sooow” wordt “soow” – geen eindeloze herhalingen), en specifieke slang-heuristieken zoals een eindigend “-uh” vervangen door “-o” (“bruh” → “bro”). Dit maakt dat variaties zoals “brooo” of “bruh” uiteindelijk dezelfde genormaliseerde vorm “bro” opleveren. De phonetic_primary en secondary kolommen bevatten de uitkomst van het Double Metaphone algoritme op de genormaliseerde vorm – dit zijn codes die woorden met vergelijkbare klank dezelfde waarde geven. Zo krijgen “bro”, “brah”, “bruh” waarschijnlijk dezelfde phonetic code. Op de words tabel staat een database trigger die bij elke insert of update automatisch normalized_word en die phonetic velden vult aan de hand van het ingevoerde woord. Hierdoor is de data altijd consistent en hoeven we in queries niet steeds opnieuw te normaliseren – het is al opgeslagen. De tweede tabel word_variants bevat alternatieve spellingen of vormen. Bijvoorbeeld bij hoofdwoord “bro” kunnen varianten zijn “bruh”, “bruv”, “bra”. Elke variant heeft een verwijzing (foreign key) naar het hoofdwoord (word_id) en eigen kolommen variant (de variant string zoals “bruh”), variant_norm (genormaliseerde variant) en phonetic codes. Ook voor deze tabel zorgen triggers dat bij toevoegen van variant de genormaliseerde variant en phonetic codes worden ingevuld. Het idee is dat zowel de hoofdwoorden als hun varianten doorzoekbaar zijn op een uniforme manier. We hebben in de database Full-text search en trigram indexes geactiveerd: op words.normalized_word en word_variants.variant_norm staat een GIN-index met pg_trgm (trigram), zodat we snel fuzzy matches kunnen vinden. Tevens hebben we indexes op de phonetic velden zodat een gelijkheid op de Double Metaphone code efficiënt is. Met deze voorbereidingen is de zoekfunctionaliteit gerealiseerd via een opgeslagen functie search_words(q text). Deze functie (geschreven in SQL/PLpgSQL) neemt een zoekquery van de gebruiker, normaliseert die query meteen op dezelfde manier (lowercase, etc.), berekent de phonetic codes van de query, en zoekt vervolgens in beide tabellen naar overeenkomsten. Er wordt gekeken naar: exacte match (genormaliseerd precies gelijk), fuzzy match (via LIKE %...% of trigram similarity boven bepaalde drempel) en fonetische match (code komt overeen). De resultaten uit words (lemma’s) en variants worden gecombineerd, waarbij ook aangegeven wordt of een hit via een variant of direct lemma kwam. Bovendien wordt een score berekend om de relevantie te rangschikken: exacte match krijgt het hoogste (score 1.0), een puur fonetische match iets lager (bijv. 0.9) tenzij er ook een goede trigram overeenkomt. De functie gebruikt greatest() om de hoogste relevantie per kandidaat te bepalen van de verschillende criteria. Uiteindelijk geeft search_words een tabel terug met kolommen: word_id, word (de hoofdvorm), meaning, example, audio_url, match_on (‘lemma’ of ‘variant’) en de score. De app kan deze RPC via Supabase aanroepen. In JavaScript bijv.: supabase.rpc('search_words', { q: gebruikerQuery }). De eerste paar resultaten (gesorteerd op score desc) worden dan gebruikt voor de auto-aanvulsuggesties en om te bepalen wat te tonen. Dankzij deze aanpak kan de gebruiker bij het intypen van bijvoorbeeld “bra” meteen resultaten zien zoals “bro (gevonden via variant bra)” of “brakka” enz., ook als hij iets niet volledig of correct spelt. Als er via phonetic een resultaat is (bijv. gebruiker typt “ouwe” en het matcht fonetisch “auw” of zo), dan kan de UI eventueel een label tonen “gevonden op basis van uitspraak”. Wanneer geen resultaten gevonden worden (scorelijst leeg), dan weet de app zeker dat het woord ontbreekt en kan hij de “Voeg dit woord toe” prompt laten zien zoals eerder genoemd. Integratie varianten in contentbeheer: In het CMS kan de beheerder bij een woord ook varianten invoeren. Bijvoorbeeld bij “bro” voegt hij toe: variant “bruh”, variant “bruv”. Deze komen in de word_variants tabel en gaan dus mee naar de app bij de volgende contentupdate. De zoekfunctie zal dan vanaf dan ook “bruh” herkennen en direct naar “bro” verwijzen. Dit sluit aan bij de ontwerpwens om alternatieve spellingen en veelvoorkomende Straat-Praatvarianten vindbaar te maken. Ook audio is op deze manier geregeld: het veld audio_url in words is bedoeld om een downloadlink naar een geluidsbestand (mp3) op te slaan. De contentbeheerder kan per woord een audio-opname uploaden (bijv. een jongere die het woord uitspreekt). De app streamt die als de gebruiker op het audio-icoon drukt. Als er voor een woord (nog) geen audio beschikbaar is, gebruikt de app de tekst-naar-spraak van het device als fallback. In de praktijk betekent dat: als audio_url leeg is of het bestand niet bestaat, roept de app de native TTS aan om het woord te spreken (in Nederlands of Engels afhankelijk van het woord – veel Straat-Praat bevat Engelse woorden, dus we kiezen dan de stem die het best past). Dit is ook geïmplementeerd: de app checkt bij het afspelen of er een audio_url is; zo ja, dan laadt hij dat mp3’tje via een audio player; zo nee, dan doet hij speak(word, language) met de system TTS. Kwaliteitsheuristieken in zoeken: Om te voorkomen dat er te veel irrelevante fuzzy hits komen (trigram search kan soms woorden teruggeven die er nauwelijks op lijken), kan de functie een drempel hanteren: bv. alleen resultaten met similarity boven 0.3 of 0.4 worden getoond. Ook worden de resultaten al gesorteerd naar relevantie: exacte matches bovenaan, dan dicht-in-de-buurt, dan verre matches. De SQL greatest(case exact, similarity, case phonetic) zorgt daarvoor. Hierin is zo gekozen dat een fonetische match een score ~0.92 krijgt, wat doorgaans hoger is dan een vage trigram match maar lager dan een echte exacte match. Zo komt bijvoorbeeld “bruh” vs “bro” (phonetic match) hoog, maar niet hoger dan als iemand exact “bro” had getypt. Deze parameters kunnen we tunen op basis van tests. We hebben de normalisatie al uitgebreid met belangrijke patronen (eind-‘h’ eraf als die na een medeklinker komt, etc.). We kunnen die functie verder uitbreiden als blijkt dat in data veel andere variaties zitten (bv. ‘y’ vs ‘i’ in spelling, “kk” als intensifier die als “k” moet tellen, etc.). Omdat het een pluggable functie is, kunnen developers hier verbeteringen doorvoeren zonder de rest van de app te raken. Voice-in en Voice-out aanvullend: Zoals eerder vermeld is spraakinput en –output een onderdeel van de functionaliteit. In de context van de zoekfunctie betekent dat: bij voice-in drukt de gebruiker op het microfoonicoon naast de zoekbalk, de app vraagt toestemming voor microfoon als dat nog niet gegeven is, en start dan de spraakherkenning in Nederlands (of eventueel auto-detect Nederlands/Engels). De herkende tekst wordt direct in de zoekbalk geplaatst en triggers dezelfde search_words query. Meestal is dit voor één woord. (De gebruiker zou ook een hele zin kunnen inspreken; in dat geval stuurt de app dat naar de AI vertaalmodule in plaats van de woordenboek zoekfunctie – dit onderscheid is er in de UI via de toggles.) Voor voice-out (uitspraak) is reeds aangegeven: de app probeert eerst een menselijke opname te gebruiken. Daarom worden in de contentupdates de audio_url’s meegegeven. Die verwijzen bijvoorbeeld naar een bestand in Supabase storage, zoals https://<storage-url>/audio/word123.mp3. De app heeft een component die dat streamt en afspeelt als de gebruiker op het luidspreker-icoon drukt. Voor woorden zonder opname roept de app de TextToSpeech API van het device aan. Dit alles is transparant voor de gebruiker – hij drukt gewoon op 🔊 en hoort het woord uitgesproken. We geven in de UI eventueel een laadindicatie als het audiofile aan het streamen is zodat de gebruiker weet dat er geluid aankomt. Voorbeeld end-to-end: Neemt de gebruiker nu de proef op de som en typt (of spreekt) “bruh” in de zoekbalk. De app normaliseert “bruh” tot “bro”, zoekt in de woordenlijst en vindt dat “bro” bestaat (als lemma) en dat “bruh” ook als variant geregistreerd staat bij “bro”. De search_words functie retourneert “bro” met hoge score, match_on variant. De app toont direct “bro – betekent: vriend (afkomstig van ‘brother’)” enz. De gebruiker ziet dus het resultaat alsof hij “bro” had gezocht. Hadden we “bruh” niet als variant gehad, dan zou de phonetic match alsnog “bro” kunnen opleveren, zij het iets minder zeker. In elk geval krijgt de gebruiker een correct antwoord. Is er geen match, dan biedt de app de mogelijkheid om het woord aan te dragen. Deze hele zoek- en vertaalfunctionaliteit is nu consistent met wat in het functioneel ontwerp beoogd was (fuzzy search is hiermee gerealiseerd).

Technische Architectuur & Uitbreidingen: De applicatie is opgezet als een monorepo met zowel de frontend (mobile app) als de backend in één project, plus de nodige configuraties voor infrastructuur. De codebase heet bijvoorbeeld “straatpraat” (werknaam van het project) en bevat een map apps/ met daarin mobile en backend, en eventueel admin voor het beheerderspaneel. De mobiele app is gebouwd in React Native (JavaScript/TypeScript) zodat deze op zowel iOS als Android draait met één codebase. Er wordt gebruikgemaakt van React Navigation voor de schermnavigatie (tabs, stacks), Redux Toolkit of context API voor state management (bv. om globale staat als profiel en punten bij te houden), en een UI-bibliotheek zoals NativeBase om consistente, toegankelijke componenten (knoppen, tekstvelden, etc.) te hebben. Voor netwerkverzoeken in de app wordt bijvoorbeeld Axios gebruikt of direct de Supabase SDK voor databasecalls. De app kan communiceren met de backend via GraphQL API’s of direct met Supabase (voor bepaalde eenvoudigere queries zoals de woordenzoekfunctie via RPC). De backend draait op Node.js met NestJS (TypeScript) als framework. NestJS is gekozen om een modulaire, gestructureerde serverapplicatie te hebben. We hebben hierin modules voor verschillende domeinen: bv. een TranslationModule, UserModule, DictionaryModule, LearningModule etc., die elk de resolvers/controllers, services en data-access bevatten voor dat stukje functionaliteit. De primaire interface die de mobiele app aanspreekt is een GraphQL API (via Apollo Server in NestJS). Dit GraphQL schema definieert queries en mutaties zoals translate(text, target), getWord(id), searchWords(query), submitContribution(word, meaning, context), getProfileData, updateSettings, etc. Hierdoor kan de app flexibel data opvragen en wordt over-fetching verminderd (de app kan precies de velden selecteren die het nodig heeft). Voor real-time notificaties of updates zou ook gebruikgemaakt kunnen worden van GraphQL Subscriptions of Supabase’s realtime mechanisme, maar in deze fase is dat nog beperkt gebruikt (pushnotificaties verlopen via de systemen van Apple/Google, niet via GraphQL). De backend is verbonden met een PostgreSQL database (de Supabase instance). NestJS gebruikt bijvoorbeeld TypeORM of Prisma als ORM/Database client om de tabellen words, word_variants, users, contributions etc. te benaderen, hoewel we in sommige gevallen ook direct via Supabase RPC werken. Voor caching van veelgebruikte data en als message-queue wordt Redis ingezet. Bijvoorbeeld de vertaalservice resultaten (AI vertalingen) kunnen kort in Redis worden bewaard, zodat als twee gebruikers kort na elkaar dezelfde zin vertalen we de tweede keer direct het antwoord hebben. Ook sessie-informatie of throttle-counters kunnen in Redis. Daarnaast gebruikt de backend een Redis-gebaseerde queue (via Bull bijvoorbeeld) om achtergrondtaken af te handelen, zoals het periodiek draaien van het dataverzamelscript of het verzenden van batch-notificaties. AI/ML integratie: Het zware AI-model voor zinsvertaling draait in een aparte service geschreven in Python (bijv. FastAPI). Deze ML-service laadt een getraind transformer model (zoals een fine-tuned mT5 of GPT-variant) en biedt een HTTP API (/translate) dat NestJS aanroept. De reden voor aparte service is dat machine learning modellen veel geheugen gebruiken en in Python makkelijker te beheren zijn met PyTorch, plus zo kan het onafhankelijk schalen. Het model zelf (stel mT5) is getraind met HuggingFace Transformers en draait via TorchServe of onnx-runtime voor performance. De NestJS backend’s TranslationService stuurt een request naar deze ML-service wanneer een GraphQL vertaalquery binnenkomt, en krijgt de vertaling + meta terug. Deze wordt dan door NestJS naar de GraphQL-response omgezet richting app. We hebben ook een Voice Service conceptueel – in de architectuurdiagram is een voice-service die spraakherkenning en -synthese zou kunnen coördineren. In de praktijk gebeurt spraakherkenning op het device (via RN libraries) en TTS ook device-native, dus die “service” is meer een module in de app dan een aparte microservice. Gebruikersbeheer: Accounts en profieldata kunnen door NestJS afgehandeld worden (bijvoorbeeld via NestJS JWT auth of via Supabase Auth). Als Supabase Auth wordt gebruikt, registreert/inlogt de app direct met Supabase (dat levert JWT’s), en de NestJS backend vertrouwt die JWT via een middleware zodat GraphQL requests geauthenticeerd zijn. De user data (punten, instellingen, enz.) staat in de PostgreSQL en is via zowel Supabase als NestJS beschikbaar. Admin panel: Er is ook een los admin-panel (bijv. een kleine React web app) waarmee content beheerders via hun browser woorden kunnen bewerken en bijdragen modereren. Deze admin-app spreekt ook de backend (GraphQL of REST endpoints met admin authenticatie) of direct Supabase if privileges are set. De projectstructuur in de monorepo bevat daarvoor een admin-panel app. Infrastructure & deployment: In de repo is een docker/ directory met Dockerfiles en configuraties om de verschillende onderdelen containerized te draaien. Er is bijvoorbeeld een Dockerfile voor de NestJS backend en één voor de ML-service. We gebruiken multi-stage builds (eerst npm ci & build, dan runtime image minimaliseren). Er zijn ook Kubernetes manifests in k8s/ map voor als we in productie Kubernetes willen gebruiken. Continuous Integration/Deployment (CI/CD) is ingericht zodat bij nieuwe commits de tests automatisch draaien en er eventueel automatisch een nieuwe build naar een server gaat. Waarschijnlijk via GitHub Actions of GitLab CI: het pipeline-script doet bijvoorbeeld npm ci, npm run build voor backend en app, en kan zelfs unit tests en linting draaien. Voor de mobiele app is er wellicht een Fastlane setup om builds naar TestFlight/Play Store te pushen. Testing: Er is aandacht besteed aan een teststrategie. Unit tests zijn geschreven voor kritieke functies, bijvoorbeeld de TranslationService (in NestJS) heeft tests die met een stub AI-service werken om te controleren of juiste API-calls en foutafhandeling gebeuren. Ook de normalize_slang functie in de database is getest met voorbeeldinput (“bruuuuh” wordt “bruuh” etc.). Verder zijn er integratietests voor de GraphQL API (bv. met een in-memory database of test schema) om te zien of queries de verwachte data teruggeven. Performance tests kunnen gedaan worden met een script dat bijv. 100 vertalingen na elkaar doet om te meten of caching werkt en hoe de responstijd is. Daarnaast zal in een later stadium gebruikerstesten plaatsvinden – met een paar ouders laten proefdraaiien – maar technisch is er al gezorgd voor telemetrie en logging zodat problemen boven water komen. Monitoring van de productie gebeurt via logging (console logs en externe monitoring service) en eventueel pings naar het health endpoint. Caching via Redis zorgt dat de app vlot blijft, bijvoorbeeld als veel gebruikers tegelijk het Woord van de Dag laden, kan dat woord en zijn vertaling even in cache staan. Uitbreiding: Generatieve zinsvertaling & Leermodule: In de loop van het project zijn twee grote uitbreidingen doorgevoerd: ten eerste is de vertaalfunctie uitgebreid van losse woorden naar volledige zinnen met behulp van een eigen ML-model (zoals mT5). Dit betekent dat de NestJS GraphQL endpoint translate nu onderscheid maakt: voor zinnen langer dan één woord schakelt hij de Python ML-inference service in, terwijl hij voor een enkel woord eventueel een direct databanklookup of simpele translatie kan doen. De resultaten van de ML-vertaling bevatten naast de vertaalde zin ook extra informatie: het model kan bijvoorbeeld alternatieve vertalingen aanbieden (andere manieren om de zin te zeggen in Straat-Praat of in formeel taalgebruik) en een korte uitleg van specifieke termen. We slaan alle output op in een translation_history tabel (met velden: gebruiker, oorspronkelijke zin, vertaling, alternatieven, uitleg, confidence, timestamp). Zo heeft de gebruiker een geschiedenis van zijn vertalingen en kunnen we later ook inzicht krijgen welke zinnen vaak vertaald worden. We hebben tevens de integratie met TTS voor zinnen gerealiseerd: na een succesvolle vertaling roept de backend een text-to-speech functie aan (bijv. Google Cloud TTS of een device TTS via de app) om een audiofragment van de vertaalde zin aan te maken. In het resultaat (GraphQL type TranslationResult) zit dan een veld audioUrl dat de app kan gebruiken om een “uitspreken” knop voor de volledige zin aan te bieden. Deze audio wordt ofwel on the fly gegenereerd en geüpload naar storage, of op verzoek via het device gedaan. Dankzij caching in Redis hoeven we dezelfde zin niet telkens opnieuw door het dure model te sturen – bij herhaalde vertaalverzoeken kan de cache geschekte resultaten teruggeven. De tweede uitbreiding is een Duolingo-achtige leermodule die losse lessen en quizzen aanbiedt los van de dagelijkse willekeur. Er is nu een nieuw tabblad “Leren” in de app met een lessenoverzicht. Elke les bevat bijvoorbeeld 5 tot 10 slangwoorden gegroepeerd thematisch of op niveau. De gebruiker kan een les kiezen (bijv. “Les 1: Straat-Praat Basis” met basiswoorden als chill, doekoe, skeer, …). De les start dan een reeks quizvragen gericht op die woordenset. Dit is geïmplementeerd in een QuizScreen component dat voor de gekozen les een lijst vragen genereert. Die vragen variëren: definitievragen (Wat betekent X?), invulzinnen met een van de leswoorden, etc., zoals eerder conceptueel beschreven. Tijdens de les-quiz worden de antwoorden van de gebruiker bijgehouden en zo nodig herhaald: als de gebruiker een vraag fout beantwoordt, wordt dat woord later in dezelfde quiz nogmaals gevraagd (eventueel in een iets andere vorm) om zeker te stellen dat de gebruiker het alsnog leert – dit is spaced repetition binnen de sessie. Bij meerdere goede antwoorden op rij kan de moeilijkheid iets stijgen, bijvoorbeeld minder hulp bij het invullen. Deze adaptiviteit binnen de les is geïnspireerd door Duolingo’s methode om fouten meteen te laten herkansen en successen uit te bouwen. Aan het eind van de les krijgt de gebruiker een resultatenoverzicht en worden punten toegekend voor het voltooien van de les (en de juiste antwoorden). Deze punten tellen op bij zijn profiel. Ook kunnen specifieke badges gekoppeld zijn aan de leermodule, bijv. “Eerste les voltooid!” of “5 dagen achtereen geleerd” (streak-badge die parallel loopt aan de Woord-van-de-Dag streak, maar nu specifiek op lessen). In het profielscherm is daarom uitgebreid: naast de algemene stats ziet de gebruiker nu ook zijn voortgang in de leermodule – bijvoorbeeld hoeveel lessen voltooid, welke laatste score voor een les, enz. De badges, punten, streaks zijn geïntegreerd zodat zowel het gebruik van losse functies (dagelijkse woorden, quiz) als de gestructureerde lessen bijdragen aan één samenhangend voortgangssysteem. Technisch betekende dit dat we nieuwe frontend-schermen toevoegden (LessonListScreen, QuizScreen) en de navigatie uitbreidden met een tab of menu-item “Leren”. In de backend/database hebben we tabellen voor lessen en lesinhoud toegevoegd of we genereren de lesinhoud on the fly uit de bestaande woordenlijst (in de eerste implementatie is het simpel: lessen zijn hard-coded lijsten van woorden in de app, later kan dit via CMS komen). De progressie (welke lessen gedaan, scores) wordt bijgehouden in de lokale opslag of in de database per gebruiker. Ook werden de bestaande systemen hergebruikt: de quiz in de leermodule gebruikt dezelfde vraaggenerator-logica als de reguliere quiz, alleen gefilterd op de leswoorden. Gamificatie sluit naadloos aan: een les voltooien triggert dezelfde punten/badge mechanieken. Overkoepelend ontwerp: De technische architectuur is daarmee modulair en schaalbaar. We hebben een mobiele app die via GraphQL en Supabase RPC’s communiceert met een NestJS backend en direct met de database waar nodig. De zware ML-taken zijn ausgelagerd aan een Python service. Data staat centraal in PostgreSQL (via Supabase voor gemak van auth en realtime). Redis zorgt voor snelheid en asynchrone taken. CI/CD zorgt dat we constant kunnen deployen en testen. En de code is gestructureerd in één repository, wat ontwikkeling efficiënt maakt (shared types en componenten tussen frontend en backend zijn mogelijk via een /packages directory met gedeelde modules). Alles is ingericht conform het hierboven beschreven functioneel ontwerp: alle concepten (vertaalfunctie, woord van de dag, quizzen, gamificatie, adaptief leren, notificaties, community content, up-to-date woordenlijst) zijn in de implementatie aanwezig en alle parameters en stromen (zoals API endpoints, input/output format, search algoritmes, feedbackloops, puntentelling, enz.) zijn duidelijk vastgelegd zodat de app-ervaring soepel en volgens ontwerp verloopt.